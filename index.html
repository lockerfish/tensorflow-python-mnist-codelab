

<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>TensorFlow and deep learning, without a PhD</title>
  <script src="./bower_components/webcomponentsjs/webcomponents-lite.min.js"></script>
  <link rel="import" href="./bower_components/codelab-elements/google-codelab-elements.html">
  <link rel="stylesheet" href="docs/css.css">
  <style>
    body {
      font-family: "Roboto",sans-serif;
    }
  </style>

</head>
<body unresolved class="fullbleed">

  <google-codelab title="TensorFlow and deep learning, without a PhD"
                  environment="web"
                  feedback-link="https://github.com/googlecodelabs/feedback/issues/new?title=[cloud-tensorflow-mnist]:&amp;labels[]=content-platform&amp;labels[]=cloud">

      <google-codelab-step label="Overview" duration="4">
        <p><img style="max-width: 624.00px" src="img/93d5f08a4f82d4c.png"></p>
<p>In this codelab, you will learn how to build and train a neural network that recognises handwritten digits. Along the way, as you enhance your neural network to achieve 99% accuracy, you will also discover the tools of the trade that deep learning professionals use to train their models efficiently.</p>
<p>This codelab uses the <a href="http://yann.lecun.com/exdb/mnist/" target="_blank">MNIST</a> dataset, a collection of 60,000 labeled digits that has kept generations of PhDs busy for almost two decades. You will solve the problem with less than 100 lines of Python / TensorFlow code.</p>
<h3 class="checklist">What you&#39;ll learn</h3>
<ul class="checklist">
<li>What is a neural network and how to train it</li>
<li>How to build a basic 1-layer neural network using TensorFlow</li>
<li>How to add more layers</li>
<li>Training tips and tricks: overfitting, dropout, learning rate decay...</li>
<li>How to troubleshoot deep neural networks</li>
<li>How to build convolutional networks</li>
</ul>
<h3>What you&#39;ll need</h3>
<ul>
<li>Python 2 or 3 (Python 3 recommended)</li>
<li>TensorFlow</li>
<li>Matplotlib (Python visualisation library)</li>
</ul>
<p>Installation instructions are given in the next step of the lab.</p>


      </google-codelab-step>

      <google-codelab-step label="Preparation: Install TensorFlow, get the sample code" duration="20">
        <p>Install the necessary software on your computer: Python, TensorFlow and Matplotlib. Full installation instructions are given here: <a href="./docs/INSTALL.txt" target="_blank">INSTALL.txt</a></p>
<p>Clone the GitHub repository:</p>
<pre>$ git clone https://github.com/martin-gorner/tensorflow-mnist-tutorial</pre>
<p>or download the ZIP file here: (for offline codelab)</p>
<pre><a href="./docs/tensorflow-mnist-tutorial-master.zip">./docs/tensorflow-mnist-tutorial-master.zip</a></pre>
<aside class="special"><p>The repository contains multiple files. The only one you will be working in is <code>mnist_1.0_softmax.py</code>. Other files are either solutions or support code for loading the data and visualising results.</p>
</aside>
<p>When you launch the initial python script, you should see a real-time visualisation of the training process:</p>
<pre>$ python3 mnist_1.0_softmax.py</pre>
<p><img style="max-width: 624.00px" src="img/ff7a33de1f55fba8.png"></p>
<p>Troubleshooting: if you cannot get the real-time visualisation to run or if you prefer working with only the text output, you can de-activate the visualisation by commenting out one line and de-commenting another. See instructions at the bottom of the file.</p>
<aside class="special"><p>The visualisation tool built for TensorFlow is <a href="https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html" target="_blank">TensorBoard</a>. Its main goal is more ambitious than what we need here. It is built so that you can follow your distributed TensorFlow jobs on remote servers. For what we need in this lab matplotlib will do and we get real-time animations as a bonus. But if you do serious work with TensorFlow, make sure you check out TensorBoard.</p>
</aside>


      </google-codelab-step>

      <google-codelab-step label="Theory: train a neural network" duration="10">
        <p>We will first watch a neural network being trained. The code is explained in the next section so you do not have to look at it now.</p>
<p>Our neural network takes in handwritten digits and classifies them, i.e. states if it recognises them as a 0, a 1, a 2 and so on up to a 9. It does so based on internal variables (&#34;weights&#34; and &#34;biases&#34;, explained later) that need to have a correct value for the classification to work well. This &#34;correct value&#34; is learned through a training process, also explained in detail later. What you need to know for now is that the training loop looks like this:</p>
<p><code>Training digits =&gt; updates to weights and biases =&gt; better recognition (loop)</code></p>
<p>Let us go through the six panels of the visualisation one by one to see what it takes to train a neural network.</p>
<p><img style="max-width: 500.00px" src="img/6a54f12d0f63c9bc.png"><img style="max-width: 213.50px" src="img/6a65407d5b2e1366.png"></p>
<p>Here you see the training digits being fed into the training loop, 100 at a time. You also see if the neural network, in its current state of training, has recognized them (white background) or mis-classified them (red background with correct label in small print on the left side, bad computed label on the right of each digit).</p>
<aside class="special"><p>There are 50,000 training digits in this dataset. We feed 100 of them into the training loop at each iteration so the system will have seen all the training digits once after 500 iterations. We call this an <strong>&#34;epoch&#34;</strong>.</p>
</aside>
<p><img style="max-width: 515.26px" src="img/e0a267f42349d949.png"><img style="max-width: 211.89px" src="img/fd1b9d682921feef.png"></p>
<p>To test the quality of the recognition in real-world conditions, we must use digits that the system has NOT seen during training. Otherwise, it could learn all the training digits by heart and still fail at recognising an &#34;8&#34; that I just wrote. The MNIST dataset contains 10,000 test digits. Here you see about 1000 of them with all the mis-recognised ones sorted at the top (on a red background). The scale on the left gives you a rough idea of the accuracy of the classifier (% of correctly recognised test digits)</p>
<p><img style="max-width: 294.50px" src="img/d8da0646c2680934.png"><img style="max-width: 215.50px" src="img/31604de1fba608c4.png"></p>
<p>To drive the training, we will define a loss function, i.e. a value representing how badly the system recognises the digits and try to minimise it. The choice of a loss function (here, &#34;cross-entropy&#34;) is explained later. What you see here is that the loss goes down on both the training and the test data as the training progresses: that is good. It means the neural network is learning. The X-axis represents iterations through the learning loop.</p>
<p><img style="max-width: 295.50px" src="img/5b261e5f3aa19e64.png"><img style="max-width: 212.50px" src="img/11cbc822729a7cb4.png"></p>
<p>The accuracy is simply the % of correctly recognised digits. This is computed both on the training and the test set. You will see it go up if the training goes well.</p>
<p><img style="max-width: 248.50px" src="img/5cb32946dd388353.png"><img style="max-width: 246.50px" src="img/f3da4b4f30228cb1.png"><img style="max-width: 214.90px" src="img/e32f69ec7c38118f.png"></p>
<p>The final two graphs represent the spread of all the values taken by the internal variables, i.e. weights and biases as the training progresses. Here you see for example that biases started at 0 initially and ended up taking values spread roughly evenly between -1.5 and 1.5. These graphs can be useful if the system does not converge well. If you see weights and biases spreading into the 100s or 1000s, you might have a problem.</p>
<p>The bands in the graphs are percentiles. There are 7 bands so each band is where 100/7=14% of all the values are.</p>
<p><code>Keyboard shortcuts for the visualisation GUI:<br>1 ......... display 1st graph only<br>2 ......... display 2nd graph only<br>3 ......... display 3rd graph only<br>4 ......... display 4th graph only<br>5 ......... display 5th graph only<br>6 ......... display 6th graph only<br>7 ......... display graphs 1 and 2<br>8 ......... display graphs 4 and 5<br>9 ......... display graphs 3 and 6<br>ESC or 0 .. back to displaying all graphs<br>SPACE ..... pause/resume<br>O ......... box zoom mode (then use mouse)<br>H ......... reset all zooms<br>Ctrl-S .... save current image</code></p>
<aside class="special"><p>What are &#34;<strong>weights</strong>&#34; and &#34;<strong>biases</strong>&#34; ? How is the &#34;<strong>cross-entropy</strong>&#34; computed ? How exactly does the training algorithm work ? Jump to the next section to find out.</p>
</aside>


      </google-codelab-step>

      <google-codelab-step label="Theory: a 1-layer neural network" duration="10">
        <p><img style="max-width: 624.00px" src="img/bdd8f1f362889583.png"></p>
<p>Handwritten digits in the MNIST dataset are 28x28 pixel greyscale images. The simplest approach for classifying them is to use the 28x28=784 pixels as inputs for a 1-layer neural network.</p>
<p><img style="max-width: 624.00px" src="img/d5222c6e3d15770a.png"></p>
<p>Each &#34;neuron&#34; in a neural network does a weighted sum of all of its inputs, adds a constant called the &#34;bias&#34; and then feeds the result through some non-linear activation function.</p>
<p>Here we design a 1-layer neural network with 10 output neurons since we want to classify digits into 10 classes (0 to 9).</p>
<p>For a classification problem, an activation function that works well is softmax. Applying softmax on a vector is done by taking the exponential of each element and then normalising the vector (using any norm, for example the ordinary euclidean length of the vector).</p>
<p><img style="max-width: 350.50px" src="img/604a9797da2a48d7.png"></p>
<aside class="special"><p>Why is &#34;softmax&#34; called softmax ? The exponential is a steeply increasing function. It will increase differences between the elements of the vector. It also quickly produces large values. Then, as you normalise the vector, the largest element, which dominates the norm, will be normalised to a value close to 1 while all the other elements will end up divided by a large value and normalised to something close to 0. The resulting vector clearly shows which was its largest element, the &#34;max&#34;, but retains the original relative order of its values, hence the &#34;soft&#34;.</p>
</aside>
<p>We will now summarise  the behaviour of this single layer of neurons into a simple formula using a matrix multiply. Let us do so directly for a &#34;mini-batch&#34; of 100 images as the input, producing 100 predictions (10-element vectors) as the output.</p>
<p><img style="max-width: 624.00px" src="img/21dabcf6d44e4d6f.png"></p>
<p>Using the first column of weights in the weights matrix W, we compute the weighted sum of all the pixels of the first image. This sum corresponds to the first neuron. Using the second column of weights, we do the same for the second neuron and so on until the 10th neuron. We can then repeat the operation for the remaining 99 images. If we call X the matrix containing our 100 images, all the weighted sums for our 10 neurons, computed on 100 images are simply X.W (matrix multiply).</p>
<p>Each neuron must now add its bias (a constant). Since we have 10 neurons, we have 10 bias constants. We will call this vector of 10 values b. It must be added to each line of the previously computed matrix. Using a bit of magic called &#34;broadcasting&#34; we will write this with a simple plus sign.</p>
<aside class="special"><p>&#34;<strong>Broadcasting</strong>&#34; is a standard trick used in Python and numpy, its scientific computation library. It extends how normal operations work on matrices with incompatible dimensions. &#34;Broadcasting add&#34; means &#34;if you are adding two matrices but you cannot because their dimensions are not compatible, try to replicate the small one as much as needed to make it work.&#34;</p>
</aside>
<p>We finally apply the softmax activation function and obtain the formula describing a 1-layer neural network, applied to 100 images:</p>
<p><img style="max-width: 624.00px" src="img/206327168bc85294.png"></p>
<aside class="special"><p>By the way, what is a &#34;<strong>tensor</strong>&#34;?<br>A &#34;tensor&#34; is like a matrix but with an arbitrary number of dimensions. A 1-dimensional tensor is a vector. A 2-dimensions tensor is a matrix. And then you can have tensors with 3, 4, 5 or more dimensions.</p>
</aside>


      </google-codelab-step>

      <google-codelab-step label="Theory: gradient descent" duration="10">
        <p>Now that our neural network produces predictions from input images, we need to measure how good they are, i.e. the distance between what the network tells us and what we know to be the truth. Remember that we have true labels for all the images in this dataset.</p>
<p>Any distance would work, the ordinary euclidian distance is fine but for classification problems one distance, called the &#34;cross-entropy&#34; is more efficient.</p>
<p><img style="max-width: 624.00px" src="img/1d8fc59e6a674f1c.png"></p>
<aside class="special"><p>&#34;<strong>One-hot</strong>&#34; encoding means that you represent the label &#34;6&#34; by using a vector of 10 values, all zeros but the 6th value which is 1. It is handy here because the format is very similar to how our neural network outputs ts predictions, also as a vector of 10 values.</p>
</aside>
<p>&#34;Training&#34; the neural network actually means using training images and labels to adjust weights and biases so as to minimise the cross-entropy loss function. Here is how it works.</p>
<p>The cross-entropy is a function of weights, biases, pixels of the training image and its known label.</p>
<p>If we compute the partial derivatives of the cross-entropy relatively to all the weights and all the biases we obtain a &#34;gradient&#34;, computed for a given image, label and present value of weights and biases. Remember that we have 7850 weights and biases so computing the gradient sounds like a lot of work. Fortunately, TensorFlow will do it for us.</p>
<p>The mathematical property of a gradient is that it points &#34;up&#34;. Since we want to go where the cross-entropy is low, we go in the opposite direction. We update weights and biases by a fraction of the gradient and do the same thing again using the next batch of training images. Hopefully, this gets us to the bottom of the pit where the cross-entropy is minimal.</p>
<p><img style="max-width: 624.00px" src="img/34e9e76c7715b719.png"></p>
<p>In this picture, cross-entropy is represented as a function of 2 weights. In reality, there are many more. The gradient descent algorithm follows the path of steepest descent into a local minimum. The training images are changed at each iteration too so that we converge towards a local minimum that works for all images.</p>
<aside class="special"><p>&#34;<strong>Learning rate</strong>&#34;: you cannot update your weights and biases by the whole length of the gradient at each iteration. It would be like trying to get to the bottom of a valley while wearing seven-league boots. You would be jumping from one side of the valley to the other. To get to the bottom, you need to do smaller steps, i.e. use only a fraction of the gradient, typically in the 1/1000th region. We call this fraction the &#34;learning rate&#34;.</p>
</aside>
<p>To sum it up, here is how the training loop looks like:</p>
<p><code>Training digits and labels =&gt; loss function =&gt; gradient (partial derivatives) =&gt; steepest descent =&gt; update weights and biases =&gt; repeat with next mini-batch of training images and labels</code></p>
<aside class="special"><p>Why work with &#34;<strong>mini-batches</strong>&#34; of 100 images and labels ?</p>
<p>You can definitely compute your gradient on just one example image and update the weights and biases immediately (it&#39;s called &#34;stochastic gradient descent&#34; in scientific literature). Doing so on 100 examples gives a gradient that better represents the constraints imposed by different example images and is therefore likely to converge towards the solution faster. The size of the mini-batch is an adjustable parameter though. There is another, more technical reason: working with batches also means working with larger matrices and these are usually easier to optimise on GPUs.</p>
</aside>
<h3 class="faq"><strong>Frequently Asked Questions</strong></h3>
<ul class="faq">
<li><a href="https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/" target="_blank">Why is the cross-entropy the right distance to use for classification problems ?</a></li>
</ul>


      </google-codelab-step>

      <google-codelab-step label="Lab: let&#39;s jump into the code" duration="15">
        <p>The code for the 1-layer neural network is already written. Please open the <code>mnist_1.0_softmax.py</code> file and follow along with the explanations.</p>
<aside class="warning"><p>Your task in this section is to understand this starting code so that you can improve it later.</p>
</aside>
<p>You should see there are only minor differences between the explanations and the starter code in the file. They correspond to functions used for the visualisation and are marked as such in comments. You can ignore them.</p>
<h3><a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py" target="_blank">mnist_1.0_softmax.py</a></h3>
<pre><code>import tensorflow as tf

X = tf.placeholder(tf.float32, [None, 28, 28, 1])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

init = tf.initialize_all_variables()</code></pre>
<p>First we define TensorFlow variables and placeholders. Variables are all the parameters that you want the training algorithm to determine for you. In our case, our weights and biases.</p>
<p>Placeholders are parameters that will be filled with actual data during training, typically training images. The shape of the tensor holding the training images is [None, 28, 28, 1] which stands for:</p>
<ul>
<li>28, 28, 1: our images are 28x28 pixels x 1 value per pixel (grayscale). The last number would be 3 for color images and is not really necessary here.</li>
<li>None: this dimension will be the number of images in the mini-batch. It will be known at training time.</li>
</ul>
<h3><a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py" target="_blank">mnist_1.0_softmax.py</a></h3>
<pre><code># model
Y = tf.nn.softmax(tf.matmul(tf.reshape(X, [-1, 784]), W) + b)
# placeholder for correct labels
Y_ = tf.placeholder(tf.float32, [None, 10])

# loss function
cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y))

# % of correct answers found in batch
is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))</code></pre>
<p>The first line is the model for our 1-layer neural network. The formula is the one we established in the previous theory section. The <code>tf.reshape</code> command transforms our 28x28 images into single vectors of 784 pixels. The &#34;-1&#34; in the reshape command means &#34;computer, figure it out, there is only one possibility&#34;. In practice it will be the number of images in a mini-batch.</p>
<p>We then need an additional placeholder for the training labels that will be provided alongside training images.</p>
<p>Now, we have model predictions and correct labels so we can compute the cross-entropy. <code>tf.reduce_sum</code> sums all the elements of a vector.</p>
<p>The last two lines compute the percentage of correctly recognised digits. They are left as an exercise for the reader to understand, using the <a href="https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html" target="_blank">TensorFlow API reference</a>. You can also skip them.</p>
<h3><a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py" target="_blank">mnist_1.0_softmax.py</a></h3>
<pre><code>optimizer = tf.train.GradientDescentOptimizer(0.003)
train_step = optimizer.minimize(cross_entropy)</code></pre>
<p>This where the TensorFlow magic happens. You select an optimiser (there are many available) and ask it to minimise the cross-entropy loss. In this step, TensorFlow computes the partial derivatives of the loss function relatively to all the weights and all the biases (the gradient). This is a formal derivation, not a numerical one which would be far too time-consuming.</p>
<p>The gradient is then used to update the weights and biases. 0.003 is the learning rate.</p>
<p>Finally, it is time to run the training loop. All the TensorFlow instructions up to this point have been preparing a computation graph in memory but nothing has been computed yet. </p>
<aside class="special"><p>TensorFlow&#39;s &#34;deferred execution&#34; model: TensorFlow was build for distributed computing. It has to know what you are going to compute, your execution graph, before it starts actually sending compute tasks to various computers. That is why it has a deferred execution model where you first use TensorFlow functions to create a computation graph in memory, then start an execution <code>Session</code> and perform actual computations using <code>Session.run</code>. At this point the graph cannot be changed anymore.</p>
<p>Thanks to this model, TensorFlow can take over a lot of the logistics of distributed computing. For example, if your instruct it to run one part of the computation on computer 1 and another part on computer 2, it can make the necessary data transfers happen automatically.</p>
</aside>
<p>The computation requires actual data to be fed into the placeholders you have defined in your TensorFlow code. This is supplied in the form of a Python dictionary where the keys are the names of the placeholders.</p>
<h3><a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py" target="_blank">mnist_1.0_softmax.py</a></h3>
<pre><code>sess = tf.Session()
sess.run(init)

for i in range(1000):
    # load batch of images and correct answers
    batch_X, batch_Y = mnist.train.next_batch(100)
    train_data={X: batch_X, Y_: batch_Y}

    # train
    sess.run(train_step, feed_dict=train_data)</code></pre>
<p>The <code>train_step</code> that is executed here was obtained when we asked TensorFlow to minimise out cross-entropy. That is the step that computes the gradient and updates weights and biases.</p>
<p>Finally, we also need to compute a couple of values for display so that we can follow how our model is performing.</p>
<p>The accuracy and cross entropy are computed on training data using this code in the training loop (every 10 iterations for example):</p>
<pre><code># success ?
a,c = sess.run([accuracy, cross_entropy], feed_dict=train_data)</code></pre>
<p>The same can be computed on test data by supplying test instead of training data in the feed dictionary (do this every 100 iterations for example. There are 10,000 test digits so this takes some CPU time):</p>
<pre><code># success on test data ?
test_data={X: mnist.test.images, Y_: mnist.test.labels}
a,c = sess.run([accuracy, cross_entropy], feed=test_data)</code></pre>
<aside class="special"><p><strong>TensorFlow and Numpy are friends</strong>: when preparing the computation graph, you only manipulate TensorFlow tensors and commands such as <code>tf.matmul</code>, <code>tf.reshape</code> and so on.</p>
<p>However, as soon as you execute a <code>Session.run</code> command, the values it returns are Numpy tensors, i.e. <code>numpy.ndarray</code> objects that can be consumed by Numpy and all the scientific comptation libraries based on it. That is how the real-time visualisation was built for this lab, using matplotlib, the standard Python plotting library, which is based on Numpy.</p>
</aside>
<p>This simple model already recognises 92% of the digits. Not bad, but you will now improve this significantly.</p>
<p><img style="max-width: 624.00px" src="img/e102f513bec53e08.png"></p>


      </google-codelab-step>

      <google-codelab-step label="Lab: adding layers" duration="10">
        <p><img style="max-width: 624.00px" src="img/a2832d28e7a4d272.png"></p>
<p>To improve the recognition accuracy we will add more layers to the neural network. The neurons in the second layer, instead of computing weighted sums of pixels will compute weighted sums of neuron outputs from the previous layer. Here is for example a 5-layer fully connected neural network:</p>
<p><img style="max-width: 624.00px" src="img/77bc41f211c9fb29.png"></p>
<p>We keep softmax as the activation function on the last layer because that is what works best for classification. On intermediate layers however we will use the the most classical activation function: the sigmoid:</p>
<p><img style="max-width: 446.50px" src="img/e5d46c389470df62.png"></p>
<aside class="warning"><p>Your task in this section is to add one or two intermediate layers to your model to increase its performance.</p>
<p>The solution can be found in file <a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.0_five_layers_sigmoid.py" target="_blank"><code>mnist_2.0_five_layers_sigmoid.py</code></a>. Use it if you are stuck only!</p>
</aside>
<p>To add a layer, you need an additional weights matrix and an additional bias vector for the intermediate layer:</p>
<pre><code>W1 = tf.Variable(tf.truncated_normal([28*28, 200] ,stddev=0.1))
B1 = tf.Variable(tf.zeros([200]))

W2 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))
B2 = tf.Variable(tf.zeros([10]))</code></pre>
<p>The shape of the weights matrix for a layer is [N, M] where N is the number of inputs and M of outputs for the layer. In the code above, we use 200 neurons in the intermediate layer and still 10 neurons in the last layer. </p>
<aside class="special"><p><strong>Tip: </strong>as you go deep, it becomes important to initialise weights with random values. The optimiser can get stuck in its initial position if you do not. <code>tf.truncated_normal</code> is a TensorFlow function that produces random values following the normal (Gaussian) distribution between -2*stddev and +2*stddev.</p>
</aside>
<p>And now change your 1-layer model into a 2-layer model:</p>
<pre><code>XX = tf.reshape(X, [-1, 28*28])

Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)
Y  = tf.nn.softmax(tf.matmul(Y1, W2) + B2)</code></pre>
<p>That&#39;s it. You should now be able to push your network above 97% accuracy with 2 intermediate layer with for example 200 and 100 neurons.</p>
<p><img style="max-width: 624.00px" src="img/dbbf4c8edae90438.png"></p>


      </google-codelab-step>

      <google-codelab-step label="Lab: special care for deep networks" duration="10">
        <p><img style="max-width: 624.00px" src="img/5cb4bd133cd34337.png"></p>
<p>As layers were added, neural networks tended to converge with more difficulties. But we know today how to make them behave. Here are a couple of 1-line updates that will help if you see an accuracy curve like this:</p>
<p><img style="max-width: 265.50px" src="img/56ac913e3330c484.png"></p>
<h2><strong>Relu activation function</strong></h2>
<p>The sigmoid activation function is actually quite problematic in deep networks. It squashes all values between 0 and 1 and when you do so repeatedly, neuron outputs and their gradients can vanish entirely. It was mentioned for historical reasons but modern networks use the RELU (Rectified Linear Unit) which looks like this:</p>
<p><img style="max-width: 399.00px" src="img/60cac06459b3cc08.png"></p>
<aside class="warning"><p>Update 1/4: replace all your sigmoids with RELUs now and you will get faster initial convergence and avoid problems later as we add layers. Simply swap <code>tf.nn.sigmoid</code> with <code>tf.nn.relu</code> in your code.</p>
</aside>
<h2><strong>A better optimizer</strong></h2>
<p>In very high dimensional spaces like here - we have in the order of 10K weights and biases - &#34;saddle points&#34; are frequent. These are points that are not local minima but where the gradient is nevertheless zero and the gradient descent optimizer stays stuck there. TensorFlow has a full array of available optimizers, including some that work with an amount of inertia and will safely sail past saddle points.</p>
<aside class="warning"><p>Update 2/4: replace your <code>tf.train.GradientDescentOptimiser</code> with a <code>tf.train.AdamOptimizer</code> now.</p>
</aside>
<h2><strong>Random initialisations</strong></h2>
<p>Accuracy still stuck at 0.1 ? Have you initialised your weights with random values ? For biases, when working with RELUs, the best practice is to initialise them to small positive values so that neurons operate in the non-zero range of the RELU initially.</p>
<pre><code>W = tf.Variable(tf.truncated_normal([K, L] ,stddev=0.1))
B = tf.Variable(tf.ones([L])/10)</code></pre>
<aside class="warning"><p>Update 3/4: check now that all your weights and biases are initialised appropriately. 0.1 as pictured above will do for biases.</p>
</aside>
<h2><strong>NaN ???</strong></h2>
<p><img style="max-width: 250.50px" src="img/796280524370a9b5.png"></p>
<p>If you see your accuracy curve crashing and the console outputting NaN for the cross-entropy, don&#39;t panic, you are attempting to compute a log(0), which is indeed Not A Number (NaN). Remember that the cross-entropy involves a log, computed on the output of the softmax layer. Since softmax is essentially an exponential, which is never zero, we should be fine but with 32 bit precision floating-point operations, exp(-100) is already a genuine zero.</p>
<p>Fortunately, TensorFlow has a handy function that computes the softmax and the cross-entropy in a single step, implemented in a numerically stable way. To use it, you will need to isolate the raw weighted sum plus bias on your last layer, before softmax is applied (&#34;logits&#34; in neural network jargon).</p>
<p>If the last line of your model was:</p>
<pre><code>Y = tf.nn.softmax(tf.matmul(Y4, W5) + B5)</code></pre>
<p>You need to replace it with:</p>
<pre><code>Ylogits = tf.matmul(Y4, W5) + B5
Y = tf.nn.softmax(Ylogits)</code></pre>
<p>And now you can compute your cross-entropy in a safe way:</p>
<pre><code>cross_entropy = tf.nn.softmax_cross_entropy_with_logits(Ylogits, Y_)</code></pre>
<p>Also add this line to bring the test and training cross-entropy to the same scale for display:</p>
<pre><code>cross_entropy = tf.reduce_mean(cross_entropy)*100</code></pre>
<aside class="warning"><p>Update 4/4: please add <code>tf.nn.softmax_cross_entropy_with_logits</code> to your code. You can also skip this step and come back to it when you actually see NaNs in your output.</p>
</aside>
<p>You are now ready to go deep.</p>


      </google-codelab-step>

      <google-codelab-step label="Lab: learning rate decay" duration="10">
        <p><img style="max-width: 624.00px" src="img/8ec267106683ff35.png"></p>
<p>With two, three or four intermediate layers, you can now get close to 98% accuracy, if you push the iterations to 5000 or beyond. But you will see that results are not very consistent.</p>
<p><img style="max-width: 624.00px" src="img/19b544d307a09804.png"></p>
<p>These curves are really noisy and look at the test accuracy: it&#39;s jumping up and down by a whole percent. This means that even with a learning rate of 0.003, we are going too fast. But we cannot just divide the learning rate by ten or the training would take forever. The good solution is to start fast and decay the learning rate exponentially to 0.0001 for example.</p>
<p>The impact of this little change is spectacular. You see that most of the noise is gone and the test accuracy is now above 98% in a sustained way.</p>
<p><img style="max-width: 624.00px" src="img/36c4ff32da84a637.png"></p>
<p>Look also at the training accuracy curve. It is now reaching 100% across several epochs (1 epoch = 500 iterations = trained on all training images once). For the first time, we are able to learn to recognise the training images perfectly.</p>
<aside class="warning"><p>Please add learning rate decay to your code. In order to pass a different learning rate to the AdamOptimizer at each iteration, you will need to define a new placeholder and feed it a new value at each iteration through <code>feed_dict</code>.</p>
<p>Here is the formula for exponential decay: <code>lr = lrmin+(lrmax-lrmin)*exp(-i/2000)</code></p>
<p>The solution can be found in file <a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.1_five_layers_relu_lrdecay.py" target="_blank"><code>mnist_2.1_five_layers_relu_lrdecay.py</code></a>. Use it if you are stuck.</p>
</aside>
<p><img style="max-width: 624.00px" src="img/9a204ebcd25ed434.png"></p>


      </google-codelab-step>

      <google-codelab-step label="Lab: dropout, overfitting" duration="10">
        <p><img style="max-width: 624.00px" src="img/681d5ef1732ef066.png"></p>
<p>You will have noticed that cross-entropy curves for test and training data start disconnecting after a couple thousand iterations. The learning algorithm works on training data only and optimises the training cross-entropy accordingly. It never sees test data so it is not surprising that after a while its work no longer has an effect on the test cross-entropy which stops dropping and sometimes even bounces back up.</p>
<p><img style="max-width: 624.00px" src="img/d1a460e8334d6b1c.png"></p>
<p>This does not immediately affect the real-world recognition capabilities of your model but it will prevent you from running many iterations and is generally a sign that the training is no longer having a positive effect. This disconnect is usually labeled &#34;overfitting&#34; and when you see it, you can try to apply a regularisation technique called &#34;dropout&#34;.</p>
<p><img style="max-width: 624.00px" src="img/5ee25552f4c216c.png"></p>
<p>In dropout, at each training iteration, you drop random neurons from the network. You choose a probability <code>pkeep</code> for a neuron to be kept, usually between 50% and 75%, and then at each iteration of the training loop, you randomly remove neurons with all their weights and biases. Different neurons will be dropped at each iteration (and you also need to boost the output of the remaining neurons in proportion to make sure activations on the next layer do not shift). When testing the performance of your network of course you put all the neurons back (<code>pkeep=1</code>).</p>
<p>TensorFlow offers a dropout function to be used on the outputs of a layer of neurons. It randomly zeroes-out some of the outputs and boosts the remaining ones by 1/pkeep. Here is how you use it in a 2-layer network:</p>
<pre><code># feed in 1 when testing, 0.75 when training
pkeep = tf.placeholder(tf.float32)

Y1 = tf.nn.relu(tf.matmul(X, W1) + B1)
Y1d = tf.nn.dropout(Y1, pkeep)

Y = tf.nn.softmax(tf.matmul(Y1d, W2) + B2)</code></pre>
<aside class="warning"><p>You can add dropout after each intermediate layer in the network now. This is an optional step in the lab, if you are pressed for time keep reading.</p>
<p>The solution can be found in file <a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.2_five_layers_relu_lrdecay_dropout.py" target="_blank"><code>mnist_2.2_five_layers_relu_lrdecay_dropout.py</code></a>. Use it if you are stuck.</p>
</aside>
<p><img style="max-width: 624.00px" src="img/600deebea5fff672.png"></p>
<p>You should see that the test loss is largely brought back under control, noise reappears (unsurprisingly given how dropout works) but in this case at least, the test accuracy remains unchanged which is a little disappointing. There must be another reason for the &#34;overfitting&#34;.</p>
<p>Before we continue, a recap of all the tools we have tried so far:</p>
<p><img style="max-width: 624.00px" src="img/b68d008cb24b363d.png"></p>
<p>Whatever we do, we do not seem to be able to break the 98% barrier in a significant way and our loss curves still exhibit the &#34;overfitting&#34; disconnect. What is really &#34;overfitting&#34; ? Overfitting happens when a neural network learns &#34;badly&#34;, in a way that works for the training examples but not so well on real-world data. There are regularisation techniques like dropout that can force it to learn in a better way but overfitting also has deeper roots.</p>
<p><img style="max-width: 624.00px" src="img/519fc815b21989cb.png"> </p>
<p>Basic overfitting happens when a neural network has too many degrees of freedom for the problem at hand. Imagine we have so many neurons that the network can store all of our training images in them and then recognise them by pattern matching. It would fail on real-world data completely. A neural network must be somewhat constrained so that it is forced to generalise what it learns during training.</p>
<p>If you have very little training data, even a small network can learn it by heart. Generally speaking, you always need lots of data to train neural networks.</p>
<p>Finally, if you have done everything well, experimented with different sizes of network to make sure its degrees of freedom are constrained, applied dropout, and trained on lots of data you might still be stuck at a performance level that nothing seems to be able to improve. This means that your neural network, in its present shape, is not capable of extracting more information from your data, as in our case here.</p>
<p>Remember how we are using our images, all pixels flattened into a single vector ? That was a really bad idea. Handwritten digits are made of shapes and we discarded the shape information when we flattened the pixels. However, there is a type of neural network that can take advantage of shape information: convolutional networks. Let us try them.</p>


      </google-codelab-step>

      <google-codelab-step label="Theory: convolutional networks" duration="15">
        <p><img style="max-width: 624.00px" src="img/53c160301db12a6e.png"></p>
<p>In a layer of a convolutional network, one &#34;neuron&#34; does a weighted sum of the pixels just above it, across a small region of the image only. It then acts normally by adding a bias and feeding the result through its activation function. The big difference is that each neuron reuses the same weights whereas in the fully-connected networks seen previously, each neuron had its own set of weights.</p>
<p>In the animation above, you can see that by sliding the patch of weights across the image in both directions (a convolution) you obtain as many output values as there were pixels in the image (some padding is necessary at the edges though).</p>
<p>To generate one plane of output values using a patch size of 4x4 and a color image as the input, as in the animation, we need 4x4x3=48 weights. That is not enough. To add more degrees of freedom, we repeat the same thing with a different set of weights.</p>
<p><img style="max-width: 448.93px" src="img/40fd4b6ad8dfb6d2.png"></p>
<p>The two (or more) sets of weights can be rewritten as one by adding a dimension to the tensor and this gives us the generic shape of the weights tensor for a convolutional layer. Since the number of input and output channels are parameters, we can start stacking and chaining convolutional layers.</p>
<p><img style="max-width: 251.50px" src="img/6eff0308ba98370e.png"></p>
<p>One last issue remains. We still need to boil the information down. In the last layer, we still want only 10 neurons for our 10 classes of digits. Traditionally, this was done by a &#34;max-pooling&#34; layer. Even if there are simpler ways today, &#34;max-pooling&#34; helps understand intuitively how convolutional networks operate: if you assume that during training, our little patches of weights evolve into filters that recognise basic shapes (horizontal and vertical lines, curves, ...) then one way of boiling useful information down is to keep through the layers the outputs where a shape was recognised with the maximum intensity. In practice, in a max-pool layer neuron outputs are processed in groups of 2x2 and only the one max one retained.</p>
<p>There is a simpler way though: if you slide the patches across the image with a stride of 2 pixels instead of 1, you also obtain fewer output values. This approach has proven just as effective and today&#39;s convolutional networks use convolutional layers only.</p>
<p><img style="max-width: 624.00px" src="img/b6b1f2c1c91158c8.png"></p>
<p>Let us build a convolutional network for handwritten digit recognition. We will use three convolutional layers at the top, our traditional softmax readout layer at the bottom and connect them with one fully-connected layer:</p>
<p><img style="max-width: 624.00px" src="img/3701df765a81a094.png"></p>
<p>Notice that the second and third convolutional layers have a stride of two which explains why they bring the number of output values down from 28x28 to 14x14 and then 7x7. The sizing of the layers is done so that the number of neurons goes down roughly by a factor of two at each layer: 28x28x14≈3000 → 14x14x8≈1500 → 7x7x12≈500 → 200. Jump to the next section for the implementation.</p>


      </google-codelab-step>

      <google-codelab-step label="Lab: a convolutional network" duration="15">
        <p><img style="max-width: 624.00px" src="img/3701df765a81a094.png"></p>
<p>To switch our code to a convolutional model, we need to define appropriate weights tensors for the convolutional layers and then add the convolutional layers to the model.</p>
<p>We have seen that a convolutional layer requires a weights tensor of the following shape. Here is the TensorFlow syntax for their initialisation:</p>
<p><img style="max-width: 215.50px" src="img/b0de36a8c59a3526.png"></p>
<pre><code>W = tf.Variable(tf.truncated_normal([4, 4, 3, 2], stddev=0.1))
B = tf.Variable(tf.ones([2])/10) # 2 is the number of output channels</code></pre>
<p>Convolutional layers can be implemented in TensorFlow using the <code>tf.nn.conv2d</code> function which performs the scanning of the input image in both directions using the supplied weights. This is only the weighted sum part of the neuron. You still need to add a bias and feed the result through an activation function.</p>
<pre><code>stride = 1  # output is still 28x28
Ycnv = tf.nn.conv2d(X, W, strides=[1, stride, stride, 1], padding=&#39;SAME&#39;)
Y = tf.nn.relu(Ycnv + B)</code></pre>
<p>Do not pay too much attention to the complex syntax for the stride. Look up the documentation for full details. The padding strategy that works here is to copy pixels from the sides of the image. All digits are on a uniform background so this just extends the background and should not add any unwanted shapes.</p>
<aside class="warning"><p>Your turn to play. Modify your model to turn it into a convolutional model. You can use the values from the drawing above to size it. You can keep your learning rate decay as it was but please remove dropout at this point.</p>
<p>The solution can be found in file <a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_3.0_convolutional.py" target="_blank"><code>mnist_3.0_convolutional.py</code></a>. Use it if you are stuck.</p>
</aside>
<p>Your model should break the 98% barrier comfortably and end up just a hair under 99%. We cannot stop so close! Look at the test cross-entropy curve. Does a solution spring to your mind ?</p>
<p><img style="max-width: 624.00px" src="img/881c4f6265de877b.png"></p>


      </google-codelab-step>

      <google-codelab-step label="Lab: the 99% challenge" duration="10">
        <p>A good approach to sizing your neural networks is to implement a network that is a little too constrained, then give it a bit more degrees of freedom and add dropout to make sure it is not overfitting. This ends up with a fairly optimal network for your problem.</p>
<p>Here for example, we used only 4 patches in the first convolutional layer. If you accept that those patches of weights evolve during training into shape recognisers, you can intuitively see that this might not be enough for our problem. Handwritten digits are mode from more than 4 elemental shapes.</p>
<p>So let us bump up the patch sizes a little, increase the number of patches in our convolutional layers from 4, 8, 12 to 6, 12, 24 and then add dropout on the fully-connected layer. Why not on the convolutional layers? Their neurons reuse the same weights, so dropout, which effectively works by freezing some weights during one training iteration, would not work on them.</p>
<p><img style="max-width: 624.00px" src="img/3490f252083904d8.png"></p>
<aside class="warning"><p>Go for it and break the 99% limit. Increase the patch sizes and channel numbers as on the picture above and add dropout on the convolutional layer.</p>
<p>The solution can be found in file <a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_3.1_convolutional_bigger_dropout.py" target="_blank"><code>mnist_3.1_convolutional_bigger_dropout.py</code></a>. Use it if you are stuck.</p>
</aside>
<p><img style="max-width: 624.00px" src="img/59472e85398457c7.png"></p>
<p>The model pictured above misses only 72 out of the 10,000 test digits. The world record, which you can find on the MNIST website is around 99.7%. We are only 0.4 percentage points away from it with our model built with 100 lines of Python / TensorFlow.</p>
<p>To finish, here is the difference dropout makes to our bigger convolutional network. Giving the neural network the additional degrees of freedom it needed bumped the final accuracy from 98.9% to 99.1%. Adding dropout not only tamed the test loss but also allowed us to sail safely above 99% and even reach 99.3%</p>
<p><img style="max-width: 624.00px" src="img/e4f164be456fcf92.png"></p>


      </google-codelab-step>

      <google-codelab-step label="Congratulations!" duration="0">
        <p>You have built your first neural network and trained it all the way to 99% accuracy. The techniques learned along the way are not specific to the MNIST dataset, actually they are very widely used when working with neural networks. As a parting gift, here is the &#34;cliff&#39;s notes&#34; card for the lab, in cartoon version. You can use it to recall what you have learned:</p>
<p><img style="max-width: 624.00px" src="img/995ef59d2fd81c84.png"></p>
<h3><strong>Next steps</strong></h3>
<ul>
<li>After fully-connected and convolutional networks, you should have a look at <a href="https://www.tensorflow.org/tutorials/recurrent/" target="_blank">recurrent neural networks</a>.</li>
<li>In this tutorial, you have learned how to build a Tensorflow model at the matrix level. Tensorflow has higher-level APIs too called <a href="https://www.tensorflow.org/tutorials/tflearn/" target="_blank">tf.learn</a>.</li>
<li>To run your training or inference in the cloud on a distributed infrastructure, we provide the <a href="https://cloud.google.com/ml/" target="_blank">Cloud ML service</a>.<br></li>
<li>Finally, we love feedback. Please tell us if you see something amiss in this lab or if you think it should be improved. We handle feedback through GitHub issues [<a href="https://github.com/googlecodelabs/feedback/issues/new?title=[cloud-tensorflow-mnist]:&labels[]=content-platform&labels[]=cloud" target="_blank">feedback link</a>].</li>
</ul>
<p><img style="max-width: 624.00px" src="img/4c2925956f9292.png"></p>
<table>
<tr><td colspan="1" rowspan="1"><p><img style="max-width: 160.00px" src="img/1dd39cb813f337e2.png"><br>The author: Martin Görner<br>Twitter: <a href="https://twitter.com/martin_gorner" target="_blank">@martin_gorner</a><br>Google +: <a href="https://plus.google.com/+MartinGorner" target="_blank">plus.google.com/+MartinGorner</a></p>
</td><td colspan="1" rowspan="1"><p><img style="max-width: 191.50px" src="img/2863687467111708.png"><br><a href="http://www.tensorflow.org" target="_blank">www.tensorflow.org</a></p>
</td></tr>
</table>
<p>All cartoon images in this lab copyright: <a href="http://fr.123rf.com/profile_alexpokusay" target="_blank">alexpokusay / 123RF stock photos</a></p>


      </google-codelab-step>

  </google-codelab>

  <script>
  </script>

</body>
</html>
